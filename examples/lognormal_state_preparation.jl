# This code is part of QuantumCircuits.
#
# (C) Copyright Rafał Pracht 2022.
#
# This code is licensed under the Apache License, Version 2.0. You may
# obtain a copy of this license in the LICENSE.txt file in the root directory
# of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
#
# Any modifications or derivative works of this code must retain this
# copyright notice, and modified files need to carry a notice indicating
# that they have been altered from the originals.

using QuantumCircuits
using QuantumCircuits.QML
using QuantumCircuits.QML.Optimization
using QuantumCircuits.Execute
using QuantumCircuits.Execute: state2probability
using QuantumCircuits.QCircuits.Circuit

using Distributions: LogNormal, quantile, cdf, pdf

using PyCall


# This code was copied from classic coding competition, it is used to check solution.
py"""
from scipy.stats import lognorm
from scipy.interpolate import interp1d
import numpy as np

TARGET_ERR = 1e-2

def l2_error(pmf: np.array, x_grid: np.array, sigma=0.1):
    pmf = np.array(pmf)
    x_grid = np.array(x_grid)
    assert all(pmf >= 0)
    assert np.isclose(sum(pmf), 1)
    assert all(x_grid >= 0)
    assert all(np.diff(x_grid) > 0)
    assert len(pmf) + 1 == len(x_grid)

    n_point = 2 ** 22
    tail_value = (TARGET_ERR / 100) ** 2
    min_x = lognorm.ppf(tail_value, sigma)
    max_x = lognorm.ppf(1 - tail_value, sigma)
    x_middle = np.linspace(min_x,max_x, n_point)
    x_lower_tail = np.linspace(0, min_x, n_point//1000)
    x_upper_tail = np.linspace(max_x, x_grid[-1], n_point//1000) if x_grid[-1] > max_x else np.array([])


    x_approx = np.diff(x_grid) / 2 + x_grid[:-1]
    x_approx = np.concatenate(([x_approx[0]], x_approx, [x_approx[-1]]) )
    pdf_approx = pmf / np.diff(x_grid)
    pdf_approx = np.concatenate(([pdf_approx[0]], pdf_approx, [pdf_approx[-1]]))

    fy = interp1d(x_approx, pdf_approx, kind='nearest', assume_sorted=True, fill_value=(0, 0), bounds_error=False)
    x_full = np.concatenate((x_lower_tail[:-1], x_middle, x_upper_tail[1:]))
    approx_pdf = fy(x_full)

    full_pdf = lognorm.pdf(x_full, sigma)
    dx = np.diff(x_full)
    dx = np.append(dx, 0)

    upper_tail_err_2_approx = lognorm.sf(x_full[-1], sigma)
    main_err_2 = sum((full_pdf - approx_pdf) ** 2 * dx)
    err = (upper_tail_err_2_approx + main_err_2) ** 0.5
    return err
"""

# Target error
TARGET_ERR = 1e-2

# Definition of target distribution
μ = 0
σ = 0.1

"Cumulative normal probability distribution"
cldf(val) = cdf(LogNormal(μ, σ), val)
mydiff(v::AbstractVector) = @views v[(begin+1):end] .- v[begin:(end-1)]
pldf(val) = pdf(LogNormal(μ, σ), val)


# Qubits
N = 9     

####
npoints = 2^N
tail_value = 0.00018 # 9
start_val = quantile(LogNormal(μ, σ), tail_value)
end_val = quantile(LogNormal(μ, σ), 1 - tail_value)


# Generate expected distribution manualy
diss_x = LinRange(start_val, end_val, npoints+1)
diss_p = mydiff(cldf.(diss_x))
diss_p = diss_p ./ sum(diss_p)


# Check the value
py"l2_error"(diss_p, diss_x) # 0.005128950543079309




  
# Generate ansact
qr = QuantumRegister(N)
cr = ClassicalRegister(N)
qc = QCircuit(qr, cr)
#add_ent_gate(i, j) = qc.cx(i, j)
add_ent_gate(i, j) = qc.u4(i, j)

#qc.x(4)
for i in 0:3
    add_ent_gate(4-i, 4-i-1)
    add_ent_gate(4+i, 4+i+1)
end

# decompose
qc = decompose(qc)
qc.measure(0:N-1, 0:N-1)

# Random parameter
params = getRandParameters(qc)
setparameters!(qc, params)



# Generate julia wrapper to call python code to check the solution
function python_l2(param, diss_x)
    start = ket"000000000"
    mat = tomatrix(qc, param)
    opt_p = qc.measures_matrix * state2probability(mat * start)

    return py"l2_error"(opt_p, diss_x)
end


# Generate expected distribution in first stage of the optimization
dx = diss_x[2] - diss_x[1]
ADDN = 2^10
const diss_y_pldf = zeros(2^N, ADDN)
for j in 1:2^N 
    for i in 0:ADDN-1
        x = diss_x[j] + i/ADDN * dx
        diss_y_pldf[j, i+1] = pldf(x)
    end
end
function loss_l2_Δ(state)
    err = 0   
    n = length(state) 
    for j in 1:n #(x_val, p_val) in zip(diss_x[1:end-1], state)
        for i in 0:ADDN-1
            err += (diss_y_pldf[j, i+1] - state[j]/dx)^2 * dx/ADDN
        end
    end
    sqrt(err)    
end    


# Generate expected distribution in second stage of the optimization when the changing the discretization is alowed
function loss_l2(state, start, Δx)
    err = 0   
    n = length(state) 
    xx = start
    for j in 1:n 
        dx = Δx[j]
        for i in 0:ADDN-1
            x = xx + i/ADDN * dx
            err += (pldf(x) - state[j]/dx)^2 * dx/ADDN
        end
        xx += dx
    end
    err += 1 - cldf(xx)
    err += cldf(start)
    sqrt(err) + max(xx - 1.75, 0) + max(0.57 - start, 0)
end


function loss_stage1(params)
    start = ket"000000000"
    mat = tomatrix(qc, params)
    state_end = state2probability(mat * start)
    
    return loss_l2_Δ(state_end)
end


# Loss function
function loss(params)
    Δx = abs.(params[2:513])
    model_param = params[514:end]
    start = ket"000000000"
    mat = tomatrix(qc, model_param)
    #state_end = qc.measures_matrix * state2probability(mat * start)
    state_end = state2probability(mat * start)
    
    #return 100*loss_mse(state_end, diss_p)
    return loss_l2(state_end, abs(params[1]), Δx)    
end





# Stege 1 - only ansact
# In the beginning, we start with random parameters. During the optimization, we save in the log the best parameters
# found till now. Bat sometimes the optimization method cannot find a better point so, in that case,
# it is good to restart it from the best parameters found lastly.
# For this we use best_param, variable.
best_param = [5.679918941917893, 4.251047958788928, 3.5437338099599174, 5.441122661263436, 2.3547800750989194, 1.2375106360535937, -5.926733578722595, 5.105113492307948, 6.220803793450637, -4.731043991920058, 1.8800031306308296, 5.5737349030831105, 3.2005996610955543, 1.7527345229272375, 2.276393940822889, 6.433743641909078, 1.8804087180413855, 1.3759654438241162, 2.111986048933823, 2.646277694072601, 5.301248156653536, -0.6540576713830272, 1.9133064975706302, 6.1010852093340455, 0.6528345760801835, 3.972915237735164, 6.886297952490527, 3.8040168359747124, 3.091485664926333, 4.931157193197006, 0.553865644097088, 6.351589535169496, 5.73496498969936, 4.631313009319303, 3.515834694977789, 1.5823482363368873, 4.984427837909524, 5.867871515378414, 3.617232165066993, 3.2714246937337843, 4.64111910105604, 5.060652198181708, 3.8212182162433823, -1.4551420517097016, -0.009585091570906114, 0.995754170516813, 5.594528591259475, 3.746195066671812, 3.608551912859001, -0.8894019646716065, 1.6715257777301376, 2.2870113766942772, 5.512166184617886, 0.5714521142705539, 3.8011358974571405, 0.4677817117962828, 4.972762238325715, -0.2699783614659865, -0.25001889490376006, 4.837342114428392, 2.5394235906884934, 3.550645631398327, 0.5985420153831956, 5.611424965417167, -0.24557751621378168, 1.3413055441945323, 3.973210486309615, 4.716470372135131, -1.4402489326761916, 0.5628093958419573, -2.07108824150549, 1.0143699305757663, 5.6810691483952125, 0.31689723006715803, 8.074224906481794, 4.0008420314440345, 1.8065031699397647, 4.351954781482624, 0.27393162181003833, 1.3946594956376743, 3.068970152437304, 0.5583909059538796, -0.08708818785242389, 3.5150056229387507, 0.2860842587147386, 2.3650804786584056, 1.5834413918199446, 6.609933255262227, 5.628757478395043, 2.2812658090447404, 4.988185892342614, 2.0926004903541244, 2.7907978362966746, 1.3885592047923723, -1.7975187312537604, 0.5383417309200358, 1.9079673632105147, 5.568083164220874, 0.3042963636675245]
params = best_param

# check if loss function works
# 0.08392833431402887
loss_stage1(params)
# check if derivative of loss function works
loss_stage1'(params)


# run the optimization to find the best parameters
val, x, itr = gradientDescent(loss_stage1, loss_stage1', params, α=0.0001, maxItr=500, debug=true,
                              useBigValInc=true, argsArePeriodic=true)
python_l2(params, diss_x)



# Stage 2 - ansact and the discretization
# In the beginning, we start with parameters found in the stage 1. 
# During the optimization, we save in the log the best parameters
# found till now. Bat sometimes the optimization method cannot find a better point so, in that case,
# it is good to restart it from the best parameters found lastly.
# For this we use best_param, variable.
#params = vcat([start_val], [dx for i in 1:512], best_param)
best_param = [0.6985367664022247, 0.0015250508769132493, 0.0014626137853927677, 0.0014590429186790747, 0.0014557643006992006, 0.001454049229170965, 0.0014152434090339097, 0.0014154645424014189, 0.0014274690355849293, 0.0014274598161225635, 0.0014514789740261806, 0.001452862041818621, 0.001452679344403947, 0.0014532633167190195, 0.0014534062224029842, 0.0014551139270538953, 0.0014566279666116142, 0.0014294643302957537, 0.0014292728552362352, 0.0014292486679427577, 0.001430919408368201, 0.0014541163023379018, 0.0014264683582781207, 0.0014242021400162903, 0.001422070773373309, 0.0014204928677025452, 0.0014179696641390485, 0.0014156865915134157, 0.0014124008315158862, 0.001408529509483185, 0.0014041771678095737, 0.0013973378725397319, 0.001388952298992998, 0.001570418557700881, 0.0014962322106539664, 0.001462427285624219, 0.0014459947314643237, 0.0014374257774495227, 0.001428716648156318, 0.0014245562365398049, 0.0014193533092526453, 0.0014024038001961888, 0.0013576144501029127, 0.0012866200181534816, 0.0013275133528779161, 0.0012857835770346154, 0.001227452723645235, 0.001111186406109634, 0.000983910846987461, 0.0027216896074014592, 0.0026272799861254514, 0.002329703282226615, 0.002179491224346476, 0.0020651851601318915, 0.001921146666865109, 0.001839085273350531, 0.0017395127606655075, 0.001667490450043708, 0.001589894352531833, 0.0015273881601382078, 0.0014604953385356485, 0.0014123153055424584, 0.001384503281780681, 0.0013010079121028586, 0.001263345763270071, 0.0019392437018189792, 0.0018531845905195134, 0.001765656258767773, 0.0016771608319892481, 0.0016207620536458761, 0.0015348904139311991, 0.001493174201442382, 0.0014363428176601125, 0.0013926572937170993, 0.0013445637548616521, 0.001303336750369339, 0.0012633468710517872, 0.0012145241282838202, 0.0012020632550880847, 0.0011534308481918403, 0.001131235941861219, 0.0017193376537336074, 0.0016597724888533748, 0.0015901715171418467, 0.0015368898036240673, 0.0015123767318647112, 0.0014356311396553619, 0.0014058761655063466, 0.0013590412156249114, 0.0013240567601352233, 0.0012954142560914297, 0.001261017421320202, 0.0012300604590274806, 0.0011888331122815257, 0.0011818315641344086, 0.0011398084522533415, 0.0011214758206358768, 0.0015952229340373536, 0.0015461925410152327, 0.0014995244580107022, 0.0014592495867959273, 0.0014333969000741467, 0.0013773370345070363, 0.001358835077933233, 0.001321376946287528, 0.0012995981533614211, 0.0012690075562490314, 0.001243308038767213, 0.0012173104928943585, 0.0011814401070210274, 0.0011799743246367661, 0.0011418620791040885, 0.001127538019388915, 0.0014900792572983812, 0.0014535935769283306, 0.001416737071400055, 0.0013869152062812814, 0.0013699774839846459, 0.0013234320726864385, 0.001311715064279069, 0.0012817918904826082, 0.0012662405975883608, 0.0012421022350219309, 0.001221652572814067, 0.0012010675891506494, 0.0011703450077255977, 0.0011732892508686432, 0.0011399300760484056, 0.0011295400629943355, 0.0016594652531698017, 0.0016230398622856263, 0.0015872650000226403, 0.0015574890611454932, 0.0015420416085757233, 0.0014928638191521655, 0.001483048503302747, 0.0014522684330481185, 0.0014376312813719342, 0.0014130008241249332, 0.0013931714162676737, 0.0013723542214696756, 0.001339787931259404, 0.0013457691869644605, 0.0013097756400954263, 0.001300341987439281, 0.0015507491008393008, 0.0015261637203704986, 0.0015041752896155628, 0.001484135126106822, 0.0014770029342604327, 0.0014374906455304132, 0.0014345485851729042, 0.001411519470600002, 0.0014030938952634524, 0.0013853572635658213, 0.001372876594962893, 0.001358128424334991, 0.0013313246231621044, 0.0013425723249070706, 0.0013116713644199353, 0.0013068903139337466, 0.0014971642717757075, 0.0014806648421181528, 0.0014637987052587196, 0.0014511946305855916, 0.0014507379280071727, 0.0014176945940493546, 0.0014216898840083323, 0.0014045104945161378, 0.0014028029289151278, 0.0013900960653398112, 0.0013815855798807312, 0.001371748170733806, 0.0013495241540608085, 0.0013660767411063943, 0.0013391527001339867, 0.0013388561546748396, 0.0014548163476431684, 0.0014442007458181228, 0.001431436572855298, 0.0014242463622673295, 0.001429384891909674, 0.0014021305361988597, 0.0014106402110206475, 0.0013987389556391816, 0.001401710245399672, 0.0013942765445974204, 0.0013899512841421967, 0.0013849156039217575, 0.0013671914382223681, 0.0013883239084301665, 0.001366084747485027, 0.0013705043752493574, 0.0013840065268518242, 0.0013794834232450104, 0.001374076911130199, 0.0013717731814049312, 0.0013821109336263964, 0.0013602654517263244, 0.0013734619280333395, 0.0013663365130791572, 0.0013737369401904806, 0.0013708579158846687, 0.001372215836762136, 0.00137146348633419, 0.0013568791977554935, 0.0013813002955968415, 0.0013651425292796244, 0.0013738576291753948, 0.0013966543411162767, 0.0013962886949657309, 0.0013977392050306366, 0.0014000265365980416, 0.00141440593165411, 0.001396724868848769, 0.0014139010343112094, 0.0014110635372021141, 0.001422288144436652, 0.0014238694581242843, 0.0014305376863160484, 0.0014344734044599609, 0.001425162292164838, 0.0014565362316065714, 0.0014417910054116323, 0.0014556711490415631, 0.0013787858094527255, 0.0013828199143807994, 0.0013859964322869632, 0.0013925654186953103, 0.0014113706622629332, 0.0013979391751956372, 0.001420571156880079, 0.0014221252718759232, 0.001438979791828751, 0.0014449032400958306, 0.0014549279104735925, 0.0014634873395270214, 0.0014583744222639685, 0.0014952136795143062, 0.001484861862948541, 0.0015039656551359283, 0.001354786320248275, 0.0013619791805626442, 0.0013662705364860031, 0.0013765310096760343, 0.0013988488689639967, 0.0013891921581173571, 0.0014148844074786226, 0.0014203947896928753, 0.0014415318137095939, 0.0014517868782907546, 0.0014652378648731226, 0.0014781177545579486, 0.0014775263477548076, 0.0015195358662817666, 0.0015144612819753395, 0.001538705231276322, 0.0012840137340744007, 0.0012938874230638791, 0.0013029288336586435, 0.0013156135454958054, 0.0013399901325483105, 0.0013337090722877268, 0.0013618681801404043, 0.001370192122697584, 0.0013933266340701044, 0.0014063998672897058, 0.001423895286844402, 0.0014398289402775231, 0.0014426524387608159, 0.0014869307509599626, 0.0014851529875569563, 0.0015127373214016589, 0.0012393131455630112, 0.001251561614344178, 0.0012641905073923548, 0.0012788580915241606, 0.0013054740210821098, 0.001303146783233153, 0.0013319773456067546, 0.0013437187834735053, 0.001367915289861407, 0.0013829186501197284, 0.0014047859142958473, 0.0014233761569990915, 0.0014287846546001967, 0.001476309139256843, 0.00147784427291122, 0.001508969458616347, 0.0012420577683725047, 0.0012569511149877653, 0.0012715060243388807, 0.0012884207691442088, 0.0013183224635017886, 0.001317831288422731, 0.0013513282156961545, 0.0013659313034867318, 0.0013956292353101637, 0.0014148868239542906, 0.001439609603830724, 0.0014626520104149816, 0.0014720890893140204, 0.00152547562292487, 0.0015320381466175936, 0.0015689304728814196, 0.0012389797986165198, 0.0012555562230620748, 0.0012704212005528214, 0.0012902424569442904, 0.0013223065125146135, 0.001324356830059311, 0.0013606476238089066, 0.0013781036744586422, 0.001410684671661639, 0.0014339894561811609, 0.0014617247081477665, 0.001488909420539718, 0.0015019785284637044, 0.001561516430633, 0.001572792516786633, 0.0016150328324987346, 0.0012622225879640489, 0.0012822064094558925, 0.0013016566469174224, 0.0013253110828427808, 0.0013615178314468801, 0.0013672753309123304, 0.0014087879513501422, 0.0014307191508915913, 0.0014690283419203894, 0.0014975723732589554, 0.0015310555032172716, 0.001564964969537854, 0.001584814514229899, 0.0016522516558236666, 0.0016695146756874769, 0.0017209848115561583, 0.0010936539804409181, 0.0011104691647516255, 0.0011299520645421525, 0.001149610347052481, 0.0011798660740155295, 0.0011834877789316535, -0.0012180712533561228, 0.001235185966170205, 0.0012650205838860226, 0.0012897330585531074, 0.001314951260909776, 0.0013441076570292988, 0.0013567260646748984, 0.001414987413240526, 0.0014282553961591391, 0.0014700810874366777, 0.00100665466146024, 0.001021092316286709, 0.0010392438265376797, 0.0010564071246929843, 0.001084735390328361, 0.0010880831965889336, 0.001119829764209365, 0.001136087660152228, 0.0011646361811603009, 0.0011851066184531333, 0.0012114419188506262, 0.0012351043376467381, 0.0012485553618770849, 0.0012988280639822246, 0.001309405279508703, 0.0013477824956327862, 0.001325569134400243, 0.001354820717448663, 0.0013845536604623742, 0.0014184147066623118, -0.0014669605978256798, 0.0014820134565243213, 0.0015383428117321405, 0.0015709900388268887, 0.0016249550312820381, 0.0016732266340927205, 0.0017270755345527521, 0.0017819416832398885, 0.0018221652613379811, 0.0019149042351733745, 0.0019589830951940107, 0.002041166820981876, 0.001049914441511213, 0.0010695306833592379, 0.0010890970281212508, 0.001111922325393364, 0.0011424092348095946, 0.001148122323619421, 0.001186850254733248, 0.0012085399722806661, 0.0012456339118897064, 0.0012740577277863866, 0.0013051377461435994, 0.0013437995213847341, 0.0013608435012981645, 0.0014261567018563652, 0.001442187100708035, 0.0014950194539597914, 0.00114555470342776, 0.0011690785285024658, 0.0011953819369324209, 0.0012224745957354897, 0.0012660644295784073, 0.0012790470563672606, 0.0013252699223236396, 0.0013590678356097331, 0.0013998231391182873, 0.0014374979624094744, 0.001479309383381721, 0.0015258755232421593, 0.0015638996161407464, 0.0016627081767713022, 0.001696824626273422, 0.0017678261017965403, 0.0012507901376389002, 0.0012819999359381866, 0.001297363796473682, 0.0013318723180596533, 0.001385852791421099, 0.0014046865444862135, 0.0014743521322658013, 0.0015037511646375202, 0.0015632301944286569, 0.0016086222995817566, 0.0016696504645476386, 0.0017420944477222488, -0.0017904681659136575, 0.0018925273282342004, 0.00192874644861548, 0.002018049717598483, 0.001253563775560509, 0.001282420843141195, 0.0013077340349358642, 0.001347245964585091, 0.0014012603142619766, 0.0014300265226613433, -0.0014918195742329803, -0.0015427727090240747, 0.001606053931367674, 0.0016548222695746712, 0.0017040828744978845, 0.0017688632895878444, 0.0018298503029674695, 0.001949157418540235, 0.0019938468062454706, 0.0020996307485186883, 0.0012967738588555982, 0.0013429560480351073, 0.0013816676067445646, -0.0014237371566038857, -0.001477898120439894, 0.0015058505177974486, 0.0015785162494333627, 0.0016363137735252462, -0.0017051362876069971, -0.0017896302948190803, -0.0018847679392540767, 0.0019409331455027597, -0.0020006945038912693, 0.0021727976557029778, -0.002273716859230789, 0.0023396047635150775, 0.0008859252345423805, 0.0009025228352248297, 0.000911962866244815, -0.0009435656796191546, 0.0009737825350755122, -0.0009854183131287981, 0.0010197613779826201, 0.0010449658441497784, 0.0010668650783340354, -0.0011031229007982025, -0.0011449295039070411, -0.0011707300436082097, 0.0012022595830936337, -0.0012449545990435287, -0.0012534255930387156, 0.0013137634698896727, 0.0009119468110754321, 0.0009339717288566699, 0.0009536434452721565, 0.0009827274065103246, 0.0010150434871248233, 0.001018913956270902, -0.0010662324719556522, 0.001068086371752219, 0.0011203838673951186, 0.001151533705969377, 0.0011954824226171228, -0.0012330488985407872, 0.0012865356517785733, -0.001299623727728437, -0.0013382247989299093, -0.0013900490044093006, -0.004867287466420086, 0.00556596746561182, 0.005815442338898269, -0.00690731301959757, 0.008085156694745178, 0.009150634244362169, 0.015804269929404896, 0.021626647423421942, 0.028145487136155216, 0.030391451106210242, 0.032856427720289384, 0.03301799389709809, 0.034801623581296376, 0.03886783120824117, 0.03887771050976683, 0.038342573698221444, 5.679770217925697, 4.251016852930093, 3.5437338099599174, 5.4414444305521865, 2.354307135256903, 1.2375106360535937, -5.926509974424452, 5.105579985273053, 6.220803793450637, -4.7314616203642075, 1.8798682031836234, 5.5737349030831105, 3.201179776528541, 1.753779628606808, 2.276393940822889, 6.4337424520833295, 1.8803472065474063, 1.37622694809517, 2.111479170265415, 2.647643433365171, 5.302181247182872, -0.6554348594430898, 1.9142676280384965, 6.10046184925552, 0.653441307827227, 3.972677408951232, 6.8859818579008865, 3.8038146362032093, 3.0912079807503754, 4.930723984116728, 0.5539812892044788, 6.3521240785694975, 5.734803410944227, 4.631396211030235, 3.5160160070405357, 1.5822384837667445, 4.984363180126585, 5.867871515378414, 3.6173393724017853, 3.2716835155406696, 4.64111910105604, 5.061027693997932, 3.821114354775824, -1.4551420517097016, -0.009680068910482396, 0.996787111078856, 5.594528591259475, 3.7461283499644282, 3.617111888578461, -0.8880106139940267, 1.67043976315479, 2.287238539333411, 5.511579494499221, 0.5714521142705539, 3.802725492932484, 0.47130547011305796, 4.97332952918446, -0.29953259079769096, -0.25001889490376006, 4.838712232639852, 2.5470329143628603, 3.549985645444483, 0.6010744636307404, 5.610669656210048, -0.24548376869623095, 1.3413055441945323, 4.032151127065874, 4.7104087155607095, -1.4537100582360183, 0.563020725313747, -2.07108824150549, 0.997181039950089, 5.681055401024746, 0.33144537477752495, 8.074229094966102, 4.037738446851733, 1.806980350762186, 4.351954781482624, 0.2745523724520492, 1.4089227530864126, 3.057200171651369, 0.5583868988591522, -0.08708818785242389, 3.5151991456138365, 0.2789465162342843, 2.3576112389327166, 1.581734903155837, 6.553041368749906, 5.636391301613432, 2.2812658090447404, 4.988148484777791, 2.1057427438462026, 2.8124362586259126, 1.383133441772288, -1.7975187312537604, 0.5343188007447871, 1.894759150030361, 5.568083164220874, 0.2691081343450627]
params = best_param

# check if loss function works
# 0.014844713200192563
loss(params)
# check if derivative of loss function works
loss'(params)


# run the optimization to find the best parameters
val, x, itr = gradientDescent(loss, loss', params, α=0.0000001, maxItr=500, debug=true,
                              useBigValInc=true, argsArePeriodic=true)
# Set the parameters into the circuits
setparameters!(qc, params[514:end])  



################################################################################
# Generate the discretization and check the final results
Δx_param = abs.(params[2:513])
opt_x = zeros(513)
opt_x[1] = abs(params[1])
for i in 1:length(Δx_param)
    opt_x[i+1] = opt_x[i] + Δx_param[i]
end
println("<$(params[1]), $(opt_x[end])>")
python_l2(params[514:end], opt_x)
# <0.6985367664022247, 1.749949851055071>
# 0.007330783173579125

# Print the discretization
# !!!! this is copied to Python file state_preparation
println(opt_x)


# Generate the Python code from the circuits
# !!!! this is copied to Python file state_preparation
println(toPythonQiskitCode(qc))

